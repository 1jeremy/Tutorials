<h2>Introduction</h2>

In the last chapter we introduced simple linear regression, which has only one independent variable. In this chapter we will learn about linear regression with multiple independent variables.

A simple linear regression model is written in the following form:
\[ Y = \alpha + \beta X + \epsilon \]

A <strong>multiple linear regression</strong> model with <em>p</em> variables is given by:
\[ Y = \alpha + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p + \epsilon \]

<h2>Python Implementation</h2>

In the last chapter we used the S&amp;P 500 index to predict Amazon stock returns. Now we will add more variables to improve our model's predictions. In particular, we shall consider Amazon's competitors.

<pre class="prettyprint linenums">import numpy as np
import pandas as pd
import quandl
import matplotlib.pyplot as plt
import statsmodels.formula.api as sm
# get stock price
quandl.ApiConfig.api_key = 'tAyfv1zpWnyhmDsp91yv'
spy_table  = quandl.get('BCIW/_SPXT')
amzn_table = quandl.get('WIKI/AMZN')
ebay_table = quandl.get('WIKI/EBAY')
wal_table  = quandl.get('WIKI/WMT')
aapl_table = quandl.get('WIKI/AAPL')</pre>
Then we take the data start from year 2016.
<pre class="prettyprint linenums">
spy  = spy_table .loc['2016',['Close']]
amzn = amzn_table.loc['2016',['Close']]
ebay = ebay_table.loc['2016',['Close']]
wal  = wal_table .loc['2016',['Close']]
aapl = aapl_table.loc['2016',['Close']]
</pre>
After taking log returns, we concat those data into a DataFrame.
<pre class="prettyprint linenums">
spy_log  = np.log(spy.Close) .diff().dropna()
amzn_log = np.log(amzn.Close).diff().dropna()
ebay_log = np.log(ebay.Close).diff().dropna()
wal_log  = np.log(wal.Close) .diff().dropna()
aapl_log = np.log(aapl.Close).diff().dropna()
df = pd.concat([spy_log,amzn_log,ebay_log,wal_log,aapl_log],axis = 1).dropna()
df.columns = ['SPY', 'AMZN', 'EBAY', 'WAL', 'AAPL']
df.tail()
</pre>

This yields the log returns of each stock in the last 5 days:

<table cellspacing="0" cellpadding="3" class="table qc-table">
    <tr>
        <td>Date</td>
        <td align="right">SPY</td>
        <td align="right">AMZN</td>
        <td align="right">EBAY</td>
        <td align="right">WAL</td>
        <td align="right">AAPL</td>
    </tr>
    <tr>
        <td>2016-12-23</td>
        <td align="right">0.001351</td>
        <td align="right">-0.007531</td>
        <td align="right">0.008427</td>
        <td align="right">-0.000719</td>
        <td align="right">0.001976</td>
    </tr>
    <tr>
        <td>2016-12-27</td>
        <td align="right">0.002254</td>
        <td align="right">0.014113</td>
        <td align="right">0.014993</td>
        <td align="right">0.002298</td>
        <td align="right">0.006331</td>
    </tr>
    <tr>
        <td>2016-12-28</td>
        <td align="right">-0.008218</td>
        <td align="right">0.000946</td>
        <td align="right">-0.007635</td>
        <td align="right">-0.005611</td>
        <td align="right">-0.004273</td>
    </tr>
    <tr>
        <td>2016-12-29</td>
        <td align="right">-0.000247</td>
        <td align="right">-0.009081</td>
        <td align="right">-0.001000</td>
        <td align="right">-0.000722</td>
        <td align="right">-0.000257</td>
    </tr>
    <tr>
        <td>2016-12-30</td>
        <td align="right">-0.004601</td>
        <td align="right">-0.020172</td>
        <td align="right">-0.009720</td>
        <td align="right">-0.002023</td>
        <td align="right">-0.007826</td>
    </tr>
</table>

As before, we use the 'statsmodels' package to perform simple linear regression:

<pre class="prettyprint linenums">simple = sm.ols(formula = 'amzn ~ spy', data = df).fit()
print simple.summary()
</pre>

<table cellspacing="0" cellpadding="3" class="table qc-table">
    <tr>
        <td>&nbsp;</td>
        <td align="right">coef</td>
        <td align="right">std err</td>
        <td align="right">t</td>
        <td align="right">P>|t|</td>
        <td align="right">[0.025</td>
        <td align="right">0.975]</td>
    </tr>
    <tr>
        <td>Intercept</td>
        <td align="right">9.876e-05</td>
        <td align="right">0.001</td>
        <td align="right">0.097</td>
        <td align="right">0.923</td>
        <td align="right">-0.002</td>
        <td align="right">0.002</td>
    </tr>
    <tr>
        <td>spy</td>
        <td align="right">1.0796</td>
        <td align="right">0.124</td>
        <td align="right">8.725</td>
        <td align="right">0.000</td>
        <td align="right">0.836</td>
        <td align="right">1.323</td>
    </tr>
</table>

Similarly, we can build a multiple linear regression model:

<pre class="prettyprint linenums">model = sm.ols(formula = 'amzn ~ spy + ebay + wal', data = df).fit()
print model.summary()
</pre>

<table cellspacing="0" cellpadding="3" class="table qc-table">
    <tr>
        <td>&nbsp;</td>
        <td align="right">coef</td>
        <td align="right">std err</td>
        <td align="right">t</td>
        <td align="right">P>|t|</td>
        <td align="right">[0.025</td>
        <td align="right">0.975]</td>
    </tr>
    <tr>
        <td>Intercept</td>
        <td align="right">0.0001</td>
        <td align="right">0.001</td>
        <td align="right">0.134</td>
        <td align="right">0.894</td>
        <td align="right">-0.002</td>
        <td align="right">0.002</td>
    </tr>
    <tr>
        <td>spy</td>
        <td align="right">1.0468</td>
        <td align="right">0.170</td>
        <td align="right">6.155</td>
        <td align="right">0.000</td>
        <td align="right">0.712</td>
        <td align="right">1.382</td>
    </tr>
    <tr>
        <td>ebay</td>
        <td align="right">-0.0795</td>
        <td align="right">0.058</td>
        <td align="right">-1.364</td>
        <td align="right">0.174</td>
        <td align="right">-0.194</td>
        <td align="right">0.035</td>
    </tr>
    <tr>
        <td>wal</td>
        <td align="right">-0.0865</td>
        <td align="right">0.089</td>
        <td align="right">-0.976</td>
        <td align="right">0.330</td>
        <td align="right">-0.261</td>
        <td align="right">0.088</td>
    </tr>
    <tr>
        <td>aapl</td>
        <td align="right">0.1529</td>
        <td align="right">0.084</td>
        <td align="right">1.831</td>
        <td align="right">0.068</td>
        <td align="right">-0.012</td>
        <td align="right">0.317</td>
    </tr>
</table>

As seen from the summary table, the p-values for Ebay, Walmart and Apple are 0.174, 0.330 and 0.068 respectively, so none of them is significant at the 95% level.

The multiple regression model has a higher \( R^2 \) than the simple one: 0.254 vs 0.234. Indeed, \( R^2 \) cannot decrease as the number of variables increases. Why? If an extra variable is added to our regression model, but it cannot account for variations in the response (amzn), then its estimated coefficient will simply be zero. It's as though that variable was never included in the model, so \( R^2 \) will not change.

However, it is not always better to add hundreds of variables or we will overfit our model. We'll talk about this in a later chapter.

Can we improve our model further? Here we try the <strong>Fama-French 5-factor model</strong>, which is an important model in asset pricing theory. We will cover it in the later tutorials.

The data is publicly available on [ref]Kenneth's Website <a href="http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/Data_Library/f-f_factors.html">Link</a>[/ref]. We have downloaded it in convenience. The following code is to fetch the data.

<pre class="prettyprint linenums">
#import libraries for remote fetch
import urllib2
from datetime import datetime
#The url for the dataset
url = 'https://www.quantconnect.com/tutorials/wp-content/uploads/2017/08/F-F_Research_Data_5_Factors_2x3_daily.csv'
response = urllib2.urlopen(url)
#read the dataset into DataFrame
fama_table = pd.read_csv(response)
#Convert time column into index
index = [datetime.strptime(str(x), "%Y%m%d") for x in fama_table.iloc[:,0]]
fama_table.index = index
#Remove time column
fama_table = fama_table.iloc[:,1:]
</pre>

After having the data, we can try the model with fama-french factors:
<pre class="prettyprint linenums">
#get data start from 2016
fama = fama_table['2016']
fama = fama.rename(columns = {'Mkt-RF':'MKT'})
#pre-process the factor data
fama = fama.apply(lambda x: x/100)
fama_df = pd.concat([fama,amzn_log],axis = 1)
#Build multiple linear regression
fama_model = sm.ols(formula = 'Close~MKT+SMB+HML+RMW+CMA',data = fama_df).fit()
print fama_model.summary()
</pre>
<img class="aligncenter size-full wp-image-2206" src="https://github.com/ShallweXiaowei/Tutorials/blob/master/Tutorial%20Series/Introduction%20to%20Financial%20Python/images/Tutorial10-fama.png?raw=true" alt="" width="600" height="450" />


The Fama-French 5-factor model has a higher \( R^2 \) of 0.387. We can compare the predictions from simple linear regression and Fama-French multiple regression by plotting them together on one chart:

<pre class="prettyprint linenums">
#put the data into a DataFrame for the convenience of plotting.
result = pd.DataFrame({'simple regression': simple.predict(),
                       'fama_french': fama_model.predict(),
                       'sample': df.amzn}, index = df.index)
#plot part, feel free to adjust the size for your better view.
plt.figure(figsize = (15,7.5))
plt.plot(result['2016-7':'2016-9'].index,result.loc['2016-7':'2016-9','simple regression'])
plt.plot(result['2016-7':'2016-9'].index,result.loc['2016-7':'2016-9','fama_french'])
plt.plot(result['2016-7':'2016-9'].index,result.loc['2016-7':'2016-9','sample'])
plt.legend()
plt.show()
</pre>
<img class="aligncenter size-full wp-image-2206" src="https://github.com/ShallweXiaowei/Tutorials/blob/master/Tutorial%20Series/Introduction%20to%20Financial%20Python/images/Tutorial10-compare.png?raw=true" alt="" width="600" height="450" />
Although it's hard to see from the chart above, the predicted return from multiple regression is closer to the actual return. Usually we don't plot the predictions to determine which model is better; we read the summary table.

<h2>Model Significance Test</h2>

Instead of using \( R^2 \) to assess whether our regression model is a good fit to the data, we can perform a hypothesis test: the F test.

The null and alternative hypotheses of an F test are:
\[ H_0: \beta_1 = \beta_2 = \dots = \beta_p = 0  \]
\[ H_1: \text{At least one coefficient is not 0} \]

We won't explain F test procedure in detail here. You just need to understand the null and alternative hypotheses. In the summary table of an F test, the 'F-statistic' is the F score, while 'prob (F-statistic)' is the p-value. Performing this test on the Fama-French model, we get a p-value of `2.21e-24` so we are almost certain that at least one of the coefficient is not 0.

If the p-value is larger than 0.05, you should consider rebuilding your model with other independent variables.

In simple linear regression, an F test is equivalent to a t test on the slope, so their p-values will be the same.

<h2>Residual Analysis</h2>
Residual analysis is one of the most important parts to examine and apply a linear model. In lots of linear model-based statistical arbitrage strategies, we use residuals to generate signals. That's why we should check the properties of residuals.

In linear regression, we assume that the mean value of the residual should be zero and the mean doesn't depend on variable X. We have an optional assumption that the error term follows normal distribution. This is for the convenience of building confidence interval, and it's true for most cases in practice.

Another import assumption is that the residual are independent and the variance of it is constant(homoskedasticity).
<h3>Normality</h3>
As we mentioned, the residual of a linear model in practice usually follows normal distribution. In other words, it has normality. We can plot the residual's density to check for normality:

<pre class="prettyprint linenums">plt.figure()
#ols.fit().model is a method to access to the residual.
fama_model.resid.plot.density()
plt.show()
</pre>
<img class="aligncenter size-full wp-image-2206" src="https://github.com/ShallweXiaowei/Tutorials/blob/master/Tutorial%20Series/Introduction%20to%20Financial%20Python/images/Tutorial10-residual.png?raw=true" alt="" width="600" height="450" />
As seen from the plot, the residual is normally distributed. By the way, the residual mean is always zero, up to machine precision:

<pre class="prettyprint linenums">print 'Residual mean: ', np.mean(fama_model.resid)
[out]:residual mean:  -2.31112163493e-16
print 'Residual variance: ', np.var(fama_model.resid)
[out]:residual variance:  0.000205113416293
</pre>

<h3>Homoskedasticity</h3>

This word is difficult to pronounce but not difficult to understand. It means that the residuals have the same variance for all values of X. Otherwise we say that 'heteroskedasticity' is detected.

<pre class="prettyprint linenums">plt.figure(figsize = (20,10))
plt.scatter(df.spy,simple.resid)
plt.axhline(0.05)
plt.axhline(-0.05)
plt.xlabel('x value')
plt.ylabel('residual')
plt.show()
</pre>
<img class="aligncenter size-full wp-image-2206" src="https://github.com/ShallweXiaowei/Tutorials/blob/master/Tutorial%20Series/Introduction%20to%20Financial%20Python/images/Tutorial10-variance.png?raw=true" alt="" width="600" height="450" />
As seen from the chart, the residuals' variance doesn't increase with X. The three outliers do not change our conclusion. Although we can plot the residuals for simple regression, we can't do this for multiple regression, so we use statsmodels to test for heteroskedasticity:

<pre class="prettyprint linenums">from statsmodels.stats import diagnostic as dia
het = dia.het_breuschpagan(fama_model.resid,fama_df[['MKT','SMB','HML','RMW','CMA']][1:])
print 'p-value: ', het[-1]
[out]:p-value of Heteroskedasticity:  0.144075842844
</pre>

No heteroskedasticity is detected at the 95% significance level.

<h2>Summary</h2>
In this chapter we have introduced multiple linear regression, F test and residual analysis, which are the fundamentals of linear models. In the next chapter we will introduce some linear algebra, which are used in modern portfolio theory and CAPM.
