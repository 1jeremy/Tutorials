<h2>Introduction</h2>

In this chapter we introduce linear algebra, which is an essential tool for portfolio optimization.

<h2>Vectors</h2>

A vector can be thought of as an arrow pointing from the origin to a specific point. Any vector or point can be represented by its coordinates i.e. an array of numbers, such as \((x,y)\) for a 2-dimensional vector, or \((x,y,z)\) for a 3-dimensional one. We usually write a vector as a column:
\[ v = \begin{pmatrix}
x \\ y \\ z
\end{pmatrix} \]

<h2>Matrices</h2>

If we have a few vectors \((v_1, v_2, \dots, v_n)\) with the same dimension, then we can put them side-by-side to form a <strong>matrix</strong>. For example, the vectors
\[ v_1 = \begin{pmatrix} 1 \\ 2 \\ 3 \end{pmatrix} \qquad
   v_2 = \begin{pmatrix} 2 \\ 2 \\ 2 \end{pmatrix} \qquad
   v_3 = \begin{pmatrix} 3 \\ 1 \\ 1 \end{pmatrix} \]
can be combined to produce a matrix:
\[ m = \begin{pmatrix}
1 &amp; 2 &amp; 3 \\
2 &amp; 2 &amp; 2 \\
3 &amp; 1 &amp; 1
\end{pmatrix} \]

m is a 3 &times; 3 matrix. We typically describe the dimensions of a matrix as \(m \times n\) where m = number of rows and n = number of columns.

<h3>Python Implementation</h3>
In Python, the NumPy package deals with linear algebra. The array we learned in the NumPy chapter can be deemed as a vector:

<pre class="prettyprint linenums">import numpy as np
a = np.array([1,2,3])
b = np.array([2,2,2])
c = np.array([3,1,1])
matrix = np.column_stack((a,b,c))
print matrix
print type(matrix)
</pre>

It is worth noticing that we used column_stack() here to ensure that the vectors are vertical and placed side-by-side to form a matrix. Without the column_stack() function, the vectors will be made horizontal and stacked on top of one another:

<pre class="prettyprint linenums">matrix2 = np.array([a,b,c])
print matrix2
</pre>

<h2>Matrix Multiplication</h2>
\(X_{ij}\) refers to a specific value in row \(i\) and column \(j\) of a matrix \(X\). For example, \(X_{23}\) is the number in the second row and third column of \(X\).

If A is an \(m \times n\) matrix and B is an \(p \times q\) matrix, and \(n=p\), we can multiply A with B, which is written as \(AB\). Please note that \(AB \neq BA\) in general i.e. matrix multiplication is <strong>not commutative</strong>. If \(q \neq m\), we cannot multiply B with A to get \(BA\) to begin with.

Consider this matrix product:
\[ AB = \begin{pmatrix}
a_{11}&amp;a_{12} \\
a_{21}&amp;a_{22}\\a_{31}&amp;a_{32}
\end{pmatrix}
\begin{pmatrix}
b_{11}&amp;b_{12} \\
b_{21}&amp;b_{22}
\end{pmatrix} = \begin{pmatrix}
x_{11}&amp;x_{12} \\
x_{21}&amp;x_{22} \\
x_{31}&amp;x_{32}
\end{pmatrix} \]

Then
\[ x_{11} = a_{11} b_{11} + a_{12} b_{12} \]
\[ x_{12} = a_{11} b_{12} + a_{12} b_{22} \]
\[ x_{21} = a_{21} b_{11} + a_{21} b_{21} \]
\[ x_{22} = a_{21} b_{12} + a_{22} b_{22} \]
\[ \dots \]

In other words, \(x_{ij}\) is the <strong>scalar product</strong> of row \(i\) from matrix \(A\) with column \(j\) from matrix \(B\).

The calculation is tedious, but fortunately we can use NumPy to do it:

<pre class="prettyprint linenums">A = np.array([[2,3],[4,2],[2,2]])
B = np.array([[4,2],[4,6]])
x = np.dot(A,B)
print x
</pre>

Using np.dot(), we can conduct matrix multiplication. A and B must be in the right order; if you reverse the order you will get an error:

<pre class="prettyprint linenums">x = np.dot(B,A)
</pre>

It's useful to remember that \(m \times n \times n \times p = m \times p\). This means an \(m \times n\) matrix multiplied with an \(n \times p\) matrix yields an \(m \times p \) matrix.
<h2>Inverse</h2>
In linear algebra, there are a group of special matrix that have \(n\times n\) dimension and with ones on the main diagonal and zeros elsewhere. This type of matrix is called <strong>identity matrix</strong>. We usually present it with \(I_n\), where \(n\) is the dimension.
\[I_n= \begin{pmatrix}
1&amp;0&amp;0&amp;...&amp;0 \\
0&amp;1&amp;0&amp;...&amp;0\\
0&amp;0&amp;1&amp;...&amp;0\\
...&amp;...&amp;...&amp;...&amp;...\\
0&amp;0&amp;0&amp;...&amp;1
\end{pmatrix}\]
If A is a \(m\times n\) matrix, the property of a identity matrix is:
\[I_mA = AI_n = A\]
We encourage you to try this equation by yourself so that you will have a deeper understanding about identity matrix and matrix multiplication.

We use identity matrix to define <strong>invertible matrix</strong>. An \(n \times n\) matrix A is called <strong>invertible</strong> if there exists an \(n \times n\) matrix B such that:
\[AB = BA = I_n\]
Then B is uniquely determined by A and is called the inverse of A, denoted by \(A^{-1}\).
We would not introduce the method to solve inverse matrix here. If you are interested in it, please refer to <a href="https://www.mathsisfun.com/algebra/matrix-inverse-row-operations-gauss-jordan.html">Gauss-Jordan method</a>.
In Python, we also use python to solve inverse matrix.
<pre class="prettyprint linenums">print matrix
print '\n-------------seperation line------------\n'
print np.linalg.inv(matrix)
</pre>
Using np.linalg.inv(), we found the inverse matrix of our variable 'matrix'. Now let's check if the multiplication is \(I\):
<pre class="prettyprint linenums">inverse = np.linalg.inv(matrix)
print np.dot(matrix,inverse)
print '\n-------------seperation line------------\n'
print np.dot(inverse,matrix)
</pre>
Not surprisingly, we ended up with an identity matrix.
If a square matrix is not invertible, it's called <strong>singular matrix</strong>.
For example, let's synthesize a singular matrix:
<pre class="prettyprint linenums">singular = np.array([[1,2,3],[1,2,3],[3,3,3]])
inv = np.linalg.inv(singular)
</pre>
We got an error message: Singular matrix.
<h2>Linear Functions</h2>
One of the most common usage of linear algebra is to solve linear equations. Consider the following linear equations:
\[2x + y - z = 8\]
\[-3x -y +2z = -11\]
\[-2x+y+2z = -3\]
If we let:
\[A= \begin{pmatrix}
2&amp;1&amp;-1 \\
-3&amp;-1&amp;2\\
-2&amp;1&amp;2
\end{pmatrix}\]
and
\[X= \begin{pmatrix}
x \\
y\\
z
\end{pmatrix}\]
and
\[B= \begin{pmatrix}
8 \\
-11\\
-3
\end{pmatrix}
\]
The above linear equations can be written as
\[A X = B\]
We strongly encourage you to do the multiplication to reproduce the linear equations so that you can have a better understanding.
If A is invertible, we can multiply \(a^{-1}\) on both side of the equation:
\[A^{-1}AX = A^{-1}B\]
Then
\[X = A^{-1}B\]
This means as long as we can find \(A^{-1}\), we can solve the linear functions!
Let's try it in python:
<pre class="prettyprint linenums">A = np.array([[2,1,-1],[-3,-1,2],[-2,1,2]])
B = np.array([[8],[-11],[-3]])
inv_A = np.linalg.inv(A)
print np.dot(inv_A,B)
</pre>
\(x = 2, y = 3, z = -1\) is the solution of the linear functions
More conveniently, we can directly use NumPy function to solve it:
<pre class="prettyprint linenums">print np.linalg.solve(A,B)
</pre>
We got exactly the same result.
We can check the correctness of the solution by substituting x, y, and z in the original linear equations.
<h2>Summary</h2>
In this chapter we briefly introduced vector, matrix, inverse matrix and solving linear equations, which is commonly used to calculate portfolio variance. It's also used to detect arbitrage opportunities by solving linear equations. Next chapter we would introduce modern portfolio theory and CAPM, which is the foundation if modern finance.
